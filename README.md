# TinyLLM

## tutorial

### Pretraining

* [LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch/tree/main)
* [MiniMind](https://github.com/jingyaogong/minimind)

#### Transformer

* [Paper: Transformers without Normalization](https://arxiv.org/abs/2503.10622)
* [Transformers without Normalization](https://jiachenzhu.github.io/DyT/)

#### MoE

* [Transformer 和 MoE 的差别](https://mp.weixin.qq.com/s/z5gpNkFkbR7nR4HHIHGx0g)
* [可视化图解 MoE 大模型的 7 个核心问题](https://mp.weixin.qq.com/s/-SFFB6gUp0KA4x95lCoxcg)

### Finetuning

#### GRPO

* [HuggingFace::Practical Exercise: Fine-tune a model with GRPO](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt)
* [Colab::Finetune LLMs with GRPO](https://colab.research.google.com/github/huggingface/notebooks/blob/main/course/en/chapter13/grpo_finetune.ipynb#scrollTo=ilrEVEdDkGgs)

### Model Evaluation

#### Instruction Follow Finetune

* [Short-answer and multiple choice benchmarks: MMLU](https://arxiv.org/abs/2009.03300)
* [Human perference comparison to other LLMs: LMSYS chatbot arena](https://arena.lmsys.org)
* [Automated conversational benchmarks: AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/)

## Usage

### Pretraining
### Finetuning
### Agent

* [OpenManus 深夜开源](https://mp.weixin.qq.com/s/Z1vtpH-Wx0QPI8MFum0Wiw)
* [mannaandpoem/OpenManus](https://github.com/mannaandpoem/OpenManus)
* [Manus 开源复刻框架 OWL，测评和使用教程](https://mp.weixin.qq.com/s/lvs2y2ZnSJo5GZ7gbVkQLQ)
* [camel-ai/owl](https://github.com/camel-ai/owl)

## others

### KV Cache

* [图解 KV Cache：解锁 LLM 推理效率的关键](https://mp.weixin.qq.com/s/uWV56N-NeHA57_UeNDE67g)

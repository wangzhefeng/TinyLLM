

* [Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)](https://lightning.ai/pages/community/tutorial/lora-llm/)
* [Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments](https://lightning.ai/pages/community/lora-insights/)
* [Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)
* [Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch](https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch)
* [rasbt/LLM-finetuning-scripts](https://github.com/rasbt/LLM-finetuning-scripts/blob/main/adapter/lora-from-scratch/lora-dora-mlp.ipynb)
* [rasbt/dora-from-scratch](https://github.com/rasbt/dora-from-scratch)

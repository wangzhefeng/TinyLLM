<details><summary>目录</summary><p>

- [理解 LLM 微调](#理解-llm-微调)
    - [微调的实际应用](#微调的实际应用)
    - [微调的优势](#微调的优势)
    - [微调的误解](#微调的误解)
    - [常见问题](#常见问题)
        - [为什么应该结合 RAG 与微调](#为什么应该结合-rag-与微调)
        - [LoRA 与 QLoRA：选择哪一个？](#lora-与-qlora选择哪一个)
        - [实验是关键](#实验是关键)
    - [模型选择](#模型选择)
- [LLM 微调技术原理](#llm-微调技术原理)
- [LLM 微调实战](#llm-微调实战)
- [LLM 微调最佳实践](#llm-微调最佳实践)
    - [关键参数](#关键参数)
</p></details><p></p>

# 理解 LLM 微调

微调 LLM 可以定制其行为，深化其领域专业知识，并优化其针对特定任务的性能。
通过使用专门数据对预训练模型（例如 Llama-3.1-8B）进行精炼，你可以：

* **更新知识**：引入模型原本未包含的新领域特定信息。
* **定制行为**：调整模型的语气、个性或响应风格，以满足特定需求或品牌声音。
* **针对任务进行优化**：提高模型在特定任务或查询中的准确性和相关性。

将微调想象成将通用模型转化为专业专家。有人争论是否应该使用检索增强生成（RAG）而不是微调，
但微调可以将知识和行为直接整合到模型中，这是 RAG 无法做到的。
在实践中，结合这两种方法能取得最佳效果——带来更高的准确性、更好的可用性和更少的幻觉。

## 微调的实际应用

微调可以应用于各种领域和需求。以下是它带来差异的几个实际例子：

* 金融领域的情感分析：训练一个 LLM 来判断一个新闻标题是否对公司产生正面或负面影响，并根据金融背景定制其理解。
* 客户支持聊天机器人：在过去的客户互动基础上进行微调，以提供更准确、个性化的回复，并符合公司的风格和术语。
* 法律文件辅助：在法律文本（合同、案例法、法规）上进行微调，用于合同分析、案例法研究或合规支持等任务，
  确保模型使用精确的法律语言。

## 微调的优势

**微调与检索增强生成：有何不同？**

微调几乎可以做检索增强生成能做的一切——但反过来就不行了。在训练过程中，微调将外部知识直接嵌入模型中。
这使得模型能够处理专业查询、总结文档，并在不依赖外部检索系统的情况下保持上下文。
但这并不意味着检索增强生成没有优势，因为它擅长从外部数据库获取最新信息。
事实上，通过微调也可以检索最新数据，但将检索增强生成与微调结合使用效率更高。

微调提供了基础模型或纯检索系统无法提供的若干显著优势：

1. **特定任务的精通**

微调将领域知识深度整合到模型中。这使得它在处理结构化、重复性或细微查询方面非常有效，
而单独使用检索增强生成的系统往往难以应对。换句话说，经过微调的模型成为其训练任务或内容的专家。

2. **摆脱检索依赖**

经过微调的模型在推理时无需依赖外部数据源。即使连接的检索系统失效或不完整，它仍然可靠，
因为所有所需信息都已经在模型的参数内部。这种自给自足意味着在生产中故障点更少。

3. **响应更快**

经过微调的模型在生成时无需调用外部知识库。跳过检索步骤意味着它们可以更快地生成答案。
这种速度使经过微调的模型成为对时间敏感的应用的理想选择，在这些应用中每一秒都至关重要。

4. **可以自定义行为和语气**

微调允许精确控制模型的沟通方式。这确保模型的响应与品牌的声音保持一致，遵守监管要求，或符合特定的语气偏好。
您将获得一个不仅知道该说什么，而且知道如何以期望的风格表达的模型。

5. **可靠的性能**

即使在同时使用微调和检索增强生成（RAG）的混合设置中，微调后的模型也能提供可靠的备用方案。
如果检索组件未能找到正确信息或返回错误数据，模型的内置知识仍然可以生成有用的回答。
这保证了您的系统更加一致和稳健的性能。

## 微调的误解

尽管微调有许多优势，但仍有一些误解存在。让我们来探讨关于微调的两个最常见的误解：

* **微调会给模型增加新的知识吗？**

是的，绝对可以。一个常见的误解是微调不会引入新的知识，但实际上它确实会。
如果你的微调数据集包含新的特定领域信息，模型在训练过程中会学习这些内容，并将其纳入其响应中。
实际上，微调可以而且确实能从零开始教会模型新的事实和模式。

* **RAG 总是比微调更好吗？**

许多人认为 RAG 将始终优于微调模型，但在微调得当的情况下，情况并非如此。
事实上，一个经过良好微调的模型在专业任务上往往能与基于 RAG 的系统匹敌，甚至超越它们。
声称“RAG 总是更好”的说法通常源于未正确配置的微调尝试：例如，使用了不正确的 LoRA 参数或训练不足。
这里有一份 Unsloth 关于 [LoRA 超参数调优](https://docs.unsloth.ai/get-started/fine-tuning-guide/lora-hyperparameters-guide) 的指南。

* **调很昂贵吗？**

尽管完整微调或预训练可能成本高昂，但这些并非必要（预训练尤其不是必要的）。在大多数情况下，LoRA 或 QLoRA 微调成本极低。

## 常见问题

### 为什么应该结合 RAG 与微调

与其在 RAG 和微调之间做选择，不如将两者结合起来以获得最佳效果。将检索系统与微调模型结合能发挥每种方法的优点。原因如下：

* **任务特定专长**：微调在专业任务或格式方面表现出色（使模型成为特定领域的专家），而 RAG 则使模型保持最新外部知识。
* **更好的适应性**：即使检索组件失效或返回不完整信息，微调模型仍能给出有用答案。
  同时，RAG 确保系统保持最新状态，无需为每价新数据重新训练模型。
* **效率**：微调为模型提供了强大的基础知识库，而 RAG 能够处理动态或快速变化的细节，无需从头进行彻底的重新训练。
  这种平衡产生了高效的流程，并降低了整体计算成本。

### LoRA 与 QLoRA：选择哪一个？

在实施微调方面，有两种流行的技术可以显著降低计算和内存需求：**LoRA** 和 **QLoRA**。
以下是每种技术的简要比较：

* **LoRA (Low-Rank Adaptation)**：仅微调一小部分额外的“适配器(adapter)”权重矩阵（以 16 位精度），
  而保持大部分原始模型不变。这显著减少了训练期间需要更新的参数数量。
* **QLoRA (Quantized LoRA)**：合 LoRA 与模型权重的 4 位(4-bit)量化，能够在极少的硬件上高效微调非常大的模型。
  通过尽可能使用 4 位(4-bit)精度，它显著降低了内存使用和计算开销。

推荐从 QLoRA 开始，因为它是最高效且最易获取的方法之一。得益于 Unsloth 的动态 4 位量化技术，
与标准 16 位 LoRA 微调相比，精度损失现在可以忽略不计。

### 实验是关键

没有单一的“最佳”微调方法——只有适用于不同场景的最佳实践。重要的是尝试不同的方法和配置，
以找到最适合您的数据集和使用场景的方案。一个很好的起点是 QLoRA（4 位），
它提供了一种非常经济、资源友好的方式来微调模型，而无需复杂的计算要求。

## 模型选择

> 指令模型(Instruct Model)还是基础模型(Base Model)？




# LLM 微调技术原理

> 大模型参数高效微调技术原理综述

1. [背景、参数高效微调简介](https://zhuanlan.zhihu.com/p/635152813)
2. [BitFit、Prefix Tuning、Prompt Tuning](https://zhuanlan.zhihu.com/p/635686756)
3. [P-Tuning、P-Tuning v2](https://zhuanlan.zhihu.com/p/635848732)
4. [Adapter Tuning 及其变体](https://zhuanlan.zhihu.com/p/636038478)
5. [LoRA、AdaLoRA、QLoRA](https://zhuanlan.zhihu.com/p/636215898)
6. [MAM Adapter、UniPELT](https://zhuanlan.zhihu.com/p/636362246)

# LLM 微调实战

> 模型参数高效微调技术实战

1. [PEFT 概述](https://zhuanlan.zhihu.com/p/651744834)
2. []()


# LLM 微调最佳实践

## 关键参数

* Learning Rate: 定义模型权重每一步训练的调整量。
    - 较高的学习率：加快训练速度，减少过拟合，但需确保不要过高，否则会导致过拟合。
    - 较低的学习率：训练更稳定，但可能需要更多轮次。
    - 典型范围：`1e-4(0.0001)` 至 `5e-5(0.00005)`。
* Epochs: 模型完整看到训练数据集的次数。
    - 推荐：1-3 个 epoch（超过 3 个 epoch 通常不是最优的，除非你希望你的模型产生更少的幻觉，
      但也意味着答案的创造性和多样性会降低）
    - 更多周期：更好的学习效果，但过拟合风险更高。
    - 较少周期：可能导致模型欠拟合。


